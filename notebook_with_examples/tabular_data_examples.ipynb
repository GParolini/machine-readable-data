{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine-Readable Data Formats\n",
    "\n",
    "## Recommendations and Best Practices for Biodiversity Informatics\n",
    "\n",
    "### ***Giuditta Parolini, Data Scientist, Museum für Naturkunde Berlin***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [Introduction](#intro)\n",
    "* [Section 1: The trouble with non-machine-readable data](#trouble)\n",
    "    * [1.1: Data published as a PDF file](#pdf)\n",
    "    * [1.2: Data published as a DOCX file](#docx)\n",
    "* [Section 2: Machine-readable data formats for tabular data](#tabular)\n",
    "    * [2.1: CSV, TSV](#csv)\n",
    "    * [2.2: TXT](#txt)\n",
    "    * [2.3: XML](#xml)\n",
    "    * [2.4: JSON](#json)\n",
    "    * [2.5: RDF](#rdf)\n",
    "    * [2.6: Parquet](#parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import json\n",
    "import pprint\n",
    "from pathlib import Path\n",
    "from docx.api import Document\n",
    "from io import StringIO\n",
    "from timeit import default_timer as timer\n",
    "from lxml import etree\n",
    "from tabula import read_pdf\n",
    "from rdflib import Graph\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a class=\"anchor\" id=\"intro\"></a>\n",
    "\n",
    "**This [Jupyter notebook](https://jupyter.org) provides practical examples that illustrate the main points discussed in the Guide on machine-readable data.**\n",
    "<br>\n",
    "\n",
    "It allows readers to see examples of the machine-readable data formats suggested, realise the challenges posed by data that are not machine-readable, and experience the pitfalls that can cause the generation of invalid files even when using machine-readable data formats like CSV. The notebook also describes how unstructured data, like digital images or other media, can be approached to provide, at least, a few pieces of machine-readable information.\n",
    "<br>\n",
    "\n",
    "Throughout the notebook, examples will be illustrated using the dataset ***Mounted Specimens of the Historical Bird Collection at the Museum für Naturkunde Berlin*** (DOI: [10.7479/wwqn-gd04](https://doi.org/10.7479/wwqn-gd04)) and modifications of it. The dataset contains metadata for over 13000 images of mounted bird specimens belonging to the bird collection of the museum. The mounted specimens have been systematically photographed and their images and related metadata are distributed under a [CC0 Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/deed.en) license. ***A copy of the original dataset is available in the GitHub repository as dataset.csv***.\n",
    "<br>\n",
    "\n",
    "**All data files used in this notebook are hosted in the data folder or can be downloaded from the Internet**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = Path.cwd() # path to the directory hosting the Jupyter notebook\n",
    "data_path = os.path.join(cwd, \"data\") # path to the data folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: The trouble with non-machine-readable data <a class=\"anchor\" id=\"trouble\"></a>\n",
    "As mentioned in the guide Introduction, PDF and DOCX files are human-readable, but not really machine-readable and extracting data from them is a challenging and error-prone exercise. An example of this will be demonstrated in this section using a PDF and a DOCX documents containing an extract of the bird collection dataset and its metadata. The content of both files is the same. It will be shown how extracting the data, which would be immediately available in a CSV file, and saving them in a machine-readable format can become a lengthy and troublesome business.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1: Data published as a PDF file <a class=\"anchor\" id=\"pdf\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PDF document here examined has a total of four pages containing a single data table preceded by a text description with the dataset metadata.\n",
    "<br>\n",
    "<br>\n",
    "![Here](pdf_overview.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python library [tabula-py](https://tabula-py.readthedocs.io/en/latest/index.html) allows to extract tables from a PDF document and save them in a machine-readable format like CSV or JSON. Reading the pdf file with tabula-py requires only a line of code, but the result needs further adjustments. The multiple line heading in the table is misinterpreted at reading time and generates two extra-rows that need to be removed manually. In addition, as the table is displayed on several pages, tabula-py interprets each page as a different table with the result that the table sections not on the first page have data rows interpreted as headings (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the pdf file with tabula-py\n",
    "df = read_pdf(os.path.join(data_path, \"PDF_doc_example.pdf\"), pages='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The multiple line heading is misinterpreted at reading time (rows 0 ad 1).\n",
    "df[0].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A new table with misleading headings starts on page 2 (and similarly for the other pages)\n",
    "df[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mis-intepretation of the table on different pages can be removed in tabula-py by changing the default value for the argument multiple_tables to False. The heading set on multiple lines can be read appropriately using the argument lattice set to True (tabula-py will not interpret line breaks as new rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_single_table = read_pdf(os.path.join(data_path, \"PDF_doc_example.pdf\"),\n",
    "                           pages='all', multiple_tables=False, lattice=True )\n",
    "df_single_table[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if the Pandas dataframe so obtained is turned into a machine-readable CSV without further preprocessing, the result will be a malformed file, as the carriage return signs (\\r) will be misinterpreted in the generation of the CSV (see right below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the dataframe to CSV\n",
    "df_single_table[0].to_csv(os.path.join(data_path, \"dataset_from_pdf.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the CSV file extracted using tabula-py looks likE when opened with a text editor:\n",
    "<br>\n",
    "![Here](invalid_csv_from_pdf.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Data published as a DOCX file <a class=\"anchor\" id=\"docx\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Python-docx](https://python-docx.readthedocs.io/en/latest/api/document.html) is a library for creating DOCX files using the Python programming language. As the library is able to create a DOCX file, it is also useful to extract content from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the document content can be extracted as a Python generator object\n",
    "content = Document(os.path.join(data_path,'DOCX_doc_example.docx')).iter_inner_content()\n",
    "content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The generator object can be unpacked in a list to see all the components in the DOCX file.\n",
    "# In our case, we have the text paragraphs and the data table\n",
    "docx_list = [el for el in content]\n",
    "docx_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is easier to consider text paragraphs separated from the data table,\n",
    "# so we remove the table from the list\n",
    "docx_list.pop(17)\n",
    "docx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the text content can be joined and printed.\n",
    "content = '\\n'.join([p.text for p in docx_list])\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One can also save this information to a TXT file for later re-use\n",
    "with open(os.path.join(data_path, \"text_extracted.txt\"), \"w\") as text_file:\n",
    "    text_file.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The table data can be extracted using a for loop and then saved into a pandas dataframe.\n",
    "# The dataframe can then be saved as a CSV file. \n",
    "# Code inspired by Stackoverflow\n",
    "# (https://stackoverflow.com/questions/46618718/python-docx-to-extract-table-from-word-docx)\n",
    "\n",
    "start = timer()\n",
    "\n",
    "document = Document(os.path.join(data_path, \"DOCX_doc_example.docx\"))\n",
    "table = document.tables[0]\n",
    "\n",
    "data = []\n",
    "\n",
    "keys = None\n",
    "for i, row in enumerate(table.rows):\n",
    "    text = (cell.text for cell in row.cells)\n",
    "\n",
    "    if i == 0:\n",
    "        keys = tuple(text)\n",
    "        continue\n",
    "    row_data = dict(zip(keys, text))\n",
    "    data.append(row_data)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "end = timer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data table extracted is the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe is well-formed and there is no issue in the data extraction, however the data have been extracted looping over the table element in the DOCX file. Loops are inefficient in Python and while in this case there is no real time issue due to the very limited table size, problems would immediately emerge when real-scale datasets with thousands of rows and tens of columns need to be extracted.\n",
    "\n",
    "The time required by the loop to run can be computed using Python [timeit](https://docs.python.org/3/library/timeit.html) library (see code cell above) and the result is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process time for extracting the data table from the DOCX file\n",
    "print(str(end - start) + \"s\", \"required to extract 312 data cells from a DOCX file\") #computed in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By contrast, reading the entire dataset in machine-readable format\n",
    "read_start = timer()\n",
    "dataset_read_from_csv = pd.read_csv(os.path.join(data_path, \"dataset.csv\"))\n",
    "read_end = timer()\n",
    "print(str(read_end - read_start) + \"s\", \"required to read in 199320 data cells (=13288rows × 15columns) from a CSV file\") #computed in seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although times remain manageable for both solutions in this case, with datasets having millions and billions of data cells the data extraction si going to become more and more time expensive making the user regret not to have the data directly available in a machine-readable format like CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Machine-readable data formats for tabular data <a class=\"anchor\" id=\"tabular\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1: CSV, TSV <a class=\"anchor\" id=\"csv\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV files might not be the solution to all data problems, but they are definitely handy for delivering tabular data in a machine-readable format. For datasets with up to 1 Million data rows they should be the first data format considered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the Python Pandas programming library reading a CSV file only takes a line code \n",
    "df_csv_comma_sep = pd.read_csv(os.path.join(data_path,\"dataset.csv\"))\n",
    "df_csv_comma_sep.head(2) #display the first two rows of the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A copy of the original dataset has been saved using the semicolon as a delimiter\n",
    "df_csv_semicolon_sep = pd.read_csv(os.path.join(data_path,\"dataset_semicolon.csv\"))\n",
    "df_csv_semicolon_sep.head(2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the expectation is to have the comma as a separator, the result is wrong, but it can be easily corrected. It is enough to specify the correct separator when reading in the data to import the dataset without issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv_semicolon_sep = pd.read_csv(os.path.join(data_path,\"dataset_semicolon.csv\"), sep=\";\")\n",
    "df_csv_semicolon_sep.head(2) # Now the dataframe is correctly read by Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly for reading the dataset in tsv format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv_tab_sep = pd.read_csv(os.path.join(data_path,\"dataset.tsv\"), sep=\"\\t\")\n",
    "df_csv_tab_sep.head(2) # Once the correct separator is specified the TSV file is read correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pandas library allows also to read in files (and even workbooks)in XLSX format. In this case, as the data table has been created properly, the dataset is also read by Pandas without issues. However, Python Pandas takes longer to read in an XLSX file compared to a CSV file with potential performance issues for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xlsx = pd.read_excel(os.path.join(data_path,\"dataset.xlsx\"))\n",
    "df_xlsx.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WARNING: The chances to create non-machine-readable files are much higher when working with spreadsheet software like Excel rather than dealing directly with the CSV data format.\n",
    "Here an example of the birds dataset formatted in Excel with added descriptions, empty cells, ect.\n",
    "![Here](invalid_dataset.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The invalid dataset above is read in without error warnings, but recovering the data requires a lengthy clean up\n",
    "of all the empty cells and of the cells that contain the dataset description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xlsx = pd.read_excel(os.path.join(data_path,\"dataset_invalid_format.xlsx\"))\n",
    "df_xlsx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A CSV file is valid even when it does not have column headers. When the headers are missing, however, the user need to check that the data analysis software is correctly interpreting the first row as a data row and not as table headings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv_no_heading = pd.read_csv(os.path.join(data_path,\"dataset_no_heading.csv\"), header=None) # header=None added to avoid the first\n",
    "                                                                        # row being considered the table header\n",
    "df_csv_no_heading.head(2) # When the headers are missing, Python Pandas just identifies the data columns\n",
    "                          # with an integer number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2: TXT <a class=\"anchor\" id=\"txt\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TXT files should be the preferred machine-readable format for unstructured and not annotated text that needs to be further analysed/mined. As an example, let's consider the text extracted from the DOCX file in [Section 1.2](#docx)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the file content\n",
    "with open(os.path.join(data_path,\"text_extracted.txt\"), \"r\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(content) # The extracted text is treated as a string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now focus on the dataset description. As we are working with plain text, there is no machine-readable indicator of where this section of text starts and finishes. We can only extract it relying on the knowledge we have of the original file structure, i.e., the dataset description is the set of words that follow the heading \"Dataset Description\" and ends before the following heading \"Keywords\". A possible way to extract the required text is to use the headings to split the text and then select the relevant part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "partition1 = \"Dataset Description\" #First partition heading\n",
    "words = content.partition(partition1) #First split at the section heading\n",
    "words_after_heading1 = content.split(partition1, 1)[1] #Selecting only the text after the first partition\n",
    "partition2 = \"Keywords\" #Second partition heading\n",
    "words_before_heading2 = words_after_heading1.split(partition2, 1)[0] #Selecting only the text after the first partition\n",
    "                                                                    # and before the second partition heading\n",
    "print(words_before_heading2) # Checking that the variable contains the required text (It does)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the text document had been provided with XML tags for the headings, it would have been much easier to extract the portion of text related to the dataset description. For instance, if there is available an XML tagged file like text_extracted.xml where the headings and the text body following the heading are tagged, it is possible to do as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the xml file\n",
    "with open(os.path.join(data_path,\"text_extracted.xml\")) as f:\n",
    "    xml = f.read()\n",
    "xml # checking that the file has been read properly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The text can be automatically transformed in a dataframe using the XML tags\n",
    "df = pd.read_xml(StringIO(xml))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset description is immediately available as a string in this case\n",
    "df[\"body\"][df.heading == \"Dataset Description\"].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3: XML <a class=\"anchor\" id=\"xml\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The content of a XML (eXtensible Markup Language) file is a combination of tags, which logically structure the content, and proper data. A version of the birds dataset in XML format will be used to illustrate the main features of the XML file format and the added features, like validation and comments, that it offers compared to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading and printing to screen the XML data \n",
    "with open (os.path.join(data_path, \"dataset.xml\")) as f:\n",
    "    content = f.readlines()\n",
    "\n",
    "for line in content[0:10]: # limited the printed sample to 10 rows\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The XML data can also be read as a dataframe, not differently than the CSV file\n",
    "df_xml = pd.read_xml(os.path.join(data_path,\"dataset.xml\")) #The dataset column that had white spaces in the heading\n",
    "                                    # needed renaming. The XML parser would otherwise throw an error\n",
    "df_xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The XML data file has the following **prolog**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the prolog only consists of the **XML declaration**. An integral part of this declaration is the document encoding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases, the XML declaration will be followed by a **Document type declaration** that specifies the root element of the document and point to a **Document type definition**, i.e., markup declarations that provide a grammar for a class of XML documents. This is for instance a public XHTML document type declaration: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n",
    "# \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When encountering this declaration, a parser tool will refer to the public document type definition linked and interpret the XML file accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The root note in the XML version of the bird dataset is data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elements of the XML file are the **rows**. Each row contains all the values for the columns in the original dataset. Each column value is enclosed within tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content[2:19] # The first row element in the XML dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, XML files can be validated. Validation can be carried out, for instance, using an XML Schema Definition (XSD). Below an example of how a snippet of the bird dataset in XML format can be validated using and XSD file using the Python lxml library and a simple function suggested by this [Stackoverflow question](https://stackoverflow.com/questions/299588/validating-with-an-xml-schema-in-python). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The structure of the XSD file\n",
    "with open (os.path.join(data_path,\"data.xsd\")) as f:\n",
    "    content = f.readlines()\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate function\n",
    "\n",
    "def validate(xml_path: str, xsd_path: str) -> bool:\n",
    "\n",
    "    xmlschema_doc = etree.parse(xsd_path)\n",
    "    xmlschema = etree.XMLSchema(xmlschema_doc)\n",
    "\n",
    "    xml_doc = etree.parse(xml_path)\n",
    "    result = xmlschema.validate(xml_doc)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation of the XML data snippet containing only the first data row\n",
    "\n",
    "if validate(os.path.join(data_path,\"dataset_snippet.xml\"), os.path.join(data_path,\"data.xsd\")):\n",
    "    print('Valid!')\n",
    "else:\n",
    "    print('Not valid!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XML files can also be validated using a Document Type Definition, in short **DTD**. The DTD can be given as a separate file with .dtd extension or embedded in the XML file. The example considered here uses the same portion (first row) of the example dataset of bird specimens with an internal DTD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The structure of the XML with internal\n",
    "with open (os.path.join(data_path,\"dataset_snippet_with_dtd.xml\")) as f:\n",
    "    content = f.readlines()\n",
    "content[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DTD is inserted right after the XML prolog. It provides information on the legal elements and attributes of the XML document and values they take:\n",
    "- ***!DOCTYPE data***: defines the root element of the XML document\n",
    "- ***!ELEMENT row (catalogue_id|key|scientific_name|title|class|family|genus|species|subspecies|collections|creation_year|absolute_url|copyright|license|authors)***: defines the elements that the root must contain\n",
    "- ***!ELEMENT catalogue_id (#PCDATA)***: defines that the element catalogue_id is of type parseable character data (PCDATA). Similar definitions are given for all the other elements that make up a row of data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The validity of the XML file is tested against the internal DTD.\n",
    "# If the verification is successful (as in this case), there is no output,\n",
    "# otherwise the parser returns an error message.\n",
    "parser = etree.XMLParser(dtd_validation=True)\n",
    "tree = etree.parse(os.path.join(data_path,\"dataset_snippet_with_dtd.xml\"), parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4: JSON <a class=\"anchor\" id=\"json\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A JSON (JavaScript Object Notation) data file consists of **objects** included within curly braces, **data** assigned as key-value pairs included within quotation marks if strings, left unquoted if numbers, and **arrays** included in square brackets. The various data elements in a JSON file are separated by **commas**.\n",
    "\n",
    "This structure can be easily traced in the image below that reproduces the first row of the example dataset used in this Jupyter notebook. The key-value pair structure is here emphasized with colours. The keys are in light blue, while the values are in red if strings, green if numbers, and deep blue if nulls or boolean values. All the key-value pairs for a data row are enclosed within a set of curly braces and each key-value pair is separated from the successive by a comma. Each data row is also separated from the following by a comma (see last line in the image).\n",
    "\n",
    "The square bracket opening at the beginning will close at the end of the file (not visible here), after the last object, i.e., the last data row, as the dataset is here reproduced as a list of JSON objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Here](json_object.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python library Pandas allows to read datasets in the JSON file format easily. The data are read as a standard Pandas dataframe, not differently from the result when reading the same data in CSV or XML format. With the JSON data format, however, it is necessary to specify how the data objects should be read. In this case, it is specified that the json objects should be considered as the dataframe rows (this is the meaning of of the argument orient=\"records\"). After the data have been read as a dataframe using the Pandas library, they can be easily analysed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the JSON data file into a dataframe\n",
    "df_json = pd.read_json(os.path.join(data_path, \"dataset.json\"), orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases, however, for performance issues or other reasons, it might be preferable to read/write a JSON data file using the [native support for JSON in the Python programming language](https://docs.python.org/3/library/json.html) rather than via the Pandas library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the JSON data file using Python json library\n",
    "with open(os.path.join(data_path, \"dataset.json\"), \"r\") as f:\n",
    "    json_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printed out to screen one has the bare structure of the JSON file with its key-value pairs, as in the image displayed at the beginning of this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the JSON data file is a list of JSON objects, once the file is read in memory, a subset of data points can be selected\n",
    "# using standard Python list syntax.\n",
    "\n",
    "json_data[0:1] # First data point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting information from the JSON data file without passing through the dataframe form is simple. For instance, a solution to extract all the unique values of the catalogue_id key is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_id_list = [] # Creating an empty list to append the catalogue_id values\n",
    "\n",
    "for el in json_data: # looping through the json data object to extract the values of the key \"catalogue_id\"\n",
    "    cat_id_list.append(el[\"catalogue_id\"])\n",
    "\n",
    "unique_cat_id_list = list(set(cat_id_list)) # the values extracted are not all unique as there are two or more images\n",
    "                                            # for each bird specimens.\n",
    "                                            # The unique values can be extracted using the using the set operation on the list\n",
    "\n",
    "print(len(unique_cat_id_list)) # unique identifiers in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the other pieces of information can be extracted from the json data object in similar fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5: RDF <a class=\"anchor\" id=\"rdf\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Resource Description Framework (RDF) is a standard for data interchange on the web. RDF files allow to exchange data using a triplet syntax (subject-predicate-object) that can be turned into a directed graph. Each triplet element is identified by a URI (Uniform Resource Identifier). We will quickly examine the properties of RDF files using an RDF version of the [*Breeding Bird Atlases*](https://catalog.data.gov/dataset/breeding-bird-atlases) dataset available on DATA.GOV and the Python library [rdflib](https://rdflib.readthedocs.io/en/stable/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Graph() # Graph object instantiated\n",
    "g.parse(\"https://data.ny.gov/api/views/vk8g-ypxi/rows.rdf?accessType=DOWNLOAD\") # rdf data file parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evident from the result of the code output, the data are now held in a graph object. The length of the graph object is the number of triplets in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The URIs associated to the elements of the triplets can be printed out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uri in g[0:3]:\n",
    "    pprint.pprint(uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph object can also be queried to extract only the predicates of the the triples (which are based on the syntax subject-predicate-object, as described above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicate_query = g.query(\"\"\"\n",
    "                     select ?predicates\n",
    "                     where {?s ?predicates ?o}\n",
    "                     \"\"\")\n",
    "\n",
    "for row in predicate_query:\n",
    "    print('%s' % row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph structure of the RDF file can be visualised graphically. The RDF dataset is way too complex to allow in full for a meaningful graphical representation, but below you can find an image of the graph generated considering only the very first portion of the dataset and the code displayed below. \n",
    "![Here](rdf_graph.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pydotplus\n",
    "from IPython.display import display, Image\n",
    "from rdflib.tools.rdf2dot import rdf2dot\n",
    "\n",
    "def visualize(g):\n",
    "    stream = io.StringIO()\n",
    "    rdf2dot(g, stream, opts = {display})\n",
    "    dg = pydotplus.graph_from_dot_data(stream.getvalue())\n",
    "    png = dg.create_png()\n",
    "    display(Image(png))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6: Parquet <a class=\"anchor\" id=\"parquet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARQUET files store data column-based rather than row-based. This is not immediately evident when reading parquet files using the Pandas library because the result is always a  Pandas dataframe.\n",
    "<br>\n",
    "Here there is an example using the dataset of the bird mounted specimens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = pd.read_csv(os.path.join(data_path, \"dataset.csv\"))\n",
    "df_csv.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet = pd.read_parquet(os.path.join(data_path, \"dataset.parquet\"), engine='fastparquet')\n",
    "df_parquet.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "val = \"8dd2e0623b1caa21c461\"\n",
    "df_csv.query(\"key == @val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "df_parquet.query(\"key == @val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance difference at query time between the CSV and the PARQUET dataset is not noticeable in this specific case. The CSV format would rather be a better choice. The dataset is still too small to make the use of PARQUET meaningful and the PARQUET version lacks associated metadata that can be used to filter and reduce response at query time.\n",
    "When working with big data that require specific formats like PARQUET, it is recommended to use the Python library [DASK](https://www.dask.org), which allows for parallel computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
